---
  layout: "post"
  title: "Can \"super-intelligent alignment\" create safer AI?"
  excerpt: "AI systems that frequently produce hallucinations will no longer win the favor of investors."
  image: "/assets/imgs/steampunk-3169877_1280.jpg"
  author: "唐鳳、江殷年"
  category: "blog"
  tags: 
    - "數位產業"
---


Recently, Huang Renxun predicted that AI will catch up with humans in the next five years. Altman, the newly returned CEO of OpenAI, also emphasized that AI development will not stop. However, its co-founder Sutskevi is worried that the next generation of AI may bring major social risks. The "Superalignment" ([Superalignment](https://openai.com/blog/introducing-superalignment)) project he leads was born for this purpose. 

To me, the so-called super-intelligent alignment is "automated alignment" so that AI can continue to meet human expectations. To understand this concept, we need to first understand the two steps of making AI, which are pre-training and fine-tuning. 

 "Pre-training" is to obtain a huge amount of data and compress it into a model. Then when inputting a request, the answer will be generated based on the uniqueness of the original data. For example, the model will notice the commonalities of faces, such as outlines and colors, and after receiving instructions, it will draw a new face. However, when training data is insufficient, specious answers may be produced, which is the so-called "AI illusion." 

 "Fine-tuning" uses a reward and punishment mechanism to allow the model to learn which answers are more acceptable to humans. For example, models should reject fabricated, harmful answers and give answers that are transparent and fully reveal confidence levels. If the relevant material is not provided in the training materials, the AI should also explain it accordingly. 

 “Super-intelligent alignment” refers to the use of existing AI systems to assist humans in completing most of the fine-tuning work. 

For example: when hearing a complex patent infringement case, the judge himself may not be an expert in the field, but as long as the matter is debated by professional lawyers from both sides, the judge can judge whether the argument complies with the regulations. The principle of super-intelligent alignment is to put humans in a position similar to that of a judge. 

Looking to the future, even by co-writing a "Constitutional AI" document, AI can automatically align with these principles - the "Collective Intelligence Project" and Anthropic have [demonstrated](https:// www.anthropic.com/index/collective-constitutional-ai-aligning-a-language-model-with-public-input). For example, a small AI trained on the Universal Declaration of Human Rights can align with the next generation of large AI, making the latter closer to what people expect. 

As policy makers, our job is to give signals to the market through policies and government investment. The recently opened AI Evaluation Center has announced the ten key points that AI should meet, namely safety, flexibility, accuracy, accountability, privacy, explainability, fairness, transparency, reliability, and information security. If you go against these ten things, although it will not be banned immediately, the government does not encourage deployment. 

Before joining the cabinet in 2016, I assisted Apple’s Siri team for six years. At that time, the most important thing for the team was accuracy and harmlessness. I believe that as alignment technology matures, AI systems that frequently produce hallucinations will no longer be favored by investors. 
 
This is like the 1980s, when humans discovered that the refrigerant used in refrigerators would destroy the ozone layer, and countries quickly signed the "Montreal Protocol" and agreed on a period of discontinuation. This is a clear signal to investors: If we don’t adjust quickly, other competing products will soon take over. On the contrary, when more stable and secure AI appears on the market for consumers to use, investment and consumption will guide the industry and avoid going astray. 
